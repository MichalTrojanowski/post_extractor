{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt ZTNBD\n",
    "\n",
    "## Lokalna instalacja\n",
    "###### Wymagania\n",
    "- docker\n",
    "- ~4GB wolnego miejsca na dysku\n",
    "- zalecany linux + build-essential\n",
    "\n",
    "###### Linux \n",
    "```bash\n",
    "make notebook\n",
    "```\n",
    "###### Others (nie sprawdzane)\n",
    "```bash\n",
    "# lokalizacja - główny katalogu projektu\n",
    "docker run -it --rm -v $(pwd):/home/jovyan/work -p 8888:8888 jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Komenda startuje kontener dockerowy z Jupyterem i podmontowuje katalog projektu.\n",
    "Moduły znajdują się w katalogu `modules` i tam też będą lądowały kolejne.\n",
    "Uruchomienie póki co możliwe jest tylko z poziomu notebook'a.\n",
    "\n",
    "+ Dokumentacja google [Google Docs](https://docs.google.com/document/d/1IylTvJbRe8s_j_bZqbM-6nWVa2IQDjQiZx-NyJsGZbg)\n",
    "\n",
    "## Moduły\n",
    "\n",
    "##### PostTransformer\n",
    "Klasa PostTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta wydobywa z kolumny inputCol, z formatu json, content i umieszcza go w kolumnie outputCol w postaci tekstu.\n",
    "##### TranslateTransformer\n",
    "Klasa TranslateTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta tłumaczy tekst zawarty w kolumnie inputCol  z języka polskiego na angielski i umieszcza go w kolumnie outputCol.\n",
    "##### SentenceTransformer\n",
    "Klasa SentenceTransformer dziedziczy po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta dzieli tekst zawarty w kolumnie inputCol  na zdania i umieszcza go w kolumnie outputCol w postaci tablicy tekstów.\n",
    "##### SpeechPartsTransformer\n",
    "Klasa SpeechPartsTransformer dziedziczy  po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta z tekstu zawartego w kolumnie inputCol  zlicza wystąpienie części mowy i wstawia do outputCol w postaci jsona.\n",
    "##### SentimentTransformer\n",
    "Klasa SentimentTransformer dziedziczy  po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta z tekstu zawartego w kolumnie inputCol  wylicza sentiment i wstawia do kolumny outputCol.\n",
    "#####  MeanFeaturesTransformer\n",
    "Klasa MeanFeaturesTransformer dziedziczy  po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Klasa ta przyjmuje dodatkowy parametr features, który zawiera listę nazw cech, które mają zostać zagregowane. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta wylicza średnie wartości dla cech podanych w parametrze features, ze wszystkich obiektów jsonowych znajdujących się w tekście zawartym w kolumnie inputCol. Wyliczone wartości wstawia do kolumny outputCol w postaci listy double’i.\n",
    "##### MedianFeaturesTransformer\n",
    "Klasa MedianFeaturesTransformer dziedziczy  po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Klasa ta przyjmuje dodatkowy parametr features, który zawiera listę nazw cech, które mają zostać zagregowane. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta wylicza medianę dla cech podanych w parametrze features, ze wszystkich obiektów jsonowych znajdujących się w tekście zawartym w kolumnie inputCol. Wyliczone wartości wstawia do kolumny outputCol w postaci listy double’i.\n",
    "##### NumberOfOccurrencesFeaturesTransformer\n",
    "Klasa NumberOfOccurrencesFeaturesTransformer dziedziczy  po klasach pyspark.ml.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol. Klasa ta przyjmuje dodatkowy parametr features, który zawiera listę nazw cech, które mają zostać zagregowane. Posiada metodę transform, która przyjmuje na wejściu obiekt typu dataframe. Metoda ta zlicza ilość wystąpień cech podanych w parametrze features, ze wszystkich obiektów jsonowych znajdujących się w tekście zawartym w kolumnie inputCol. Wyliczone wartości wstawia do kolumny outputCol w postaci listy double’i.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład użycia\n",
    "\n",
    "### Instalacja Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicjalizacja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from modules.posts import (\n",
    "    SentenceTransformer, PostTransformer, TranslateTransformer,\n",
    "    SpeechPartsTransformer, BasicSpeechPartsTransformer, SentimentTransformer\n",
    ")\n",
    "from modules.features_ import (\n",
    "    MaxFeaturesTransformer,\n",
    "    MeanFeaturesTransformer,\n",
    "    MedianFeaturesTransformer,\n",
    "    NumberOfOccurrencesFeaturesTransformer,\n",
    "    FeatureTransformer,\n",
    "    SelectFeaturesTransformer\n",
    ")\n",
    "\n",
    "sc = SparkContext('local[*]', 'PipelineFlow')\n",
    "sess = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie plików"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(spark_ctx, files):\n",
    "    rdd = spark_ctx.wholeTextFiles(files)\n",
    "    rdd = rdd.map(lambda x: (x[0], x[1]))\n",
    "    df = rdd.toDF(['file', 'content'])\n",
    "    return df\n",
    "\n",
    "def load_posts(spark_ctx, files):\n",
    "    rdd = spark_ctx.wholeTextFiles(files)\n",
    "    rdd = rdd.map(lambda x: (x[0], json.loads(x[1])))\n",
    "    df = rdd.toDF(['file', 'content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykłady użycia transformerów z pipeline'ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(features_as_df):\n",
    "    \n",
    "    features = [\n",
    "        \"leaf\",\n",
    "        \"has-attribute-class\",\n",
    "    ]\n",
    "    \n",
    "    featurer = FeatureTransformer();\n",
    "    featurer.setInputCol('content').setOutputCol('features')\n",
    "    \n",
    "    feature_selector = SelectFeaturesTransformer(features=features)\n",
    "    feature_selector.setInputCol('features').setOutputCol('selected_features')\n",
    "    \n",
    "    max_feature_transformer = MaxFeaturesTransformer()\n",
    "    max_feature_transformer.setInputCol('selected_features').setOutputCol('max')\n",
    "    \n",
    "    mean_feature_transformer = MeanFeaturesTransformer()\n",
    "    mean_feature_transformer.setInputCol('selected_features').setOutputCol('mean')\n",
    "    \n",
    "    median_feature_transformer = MedianFeaturesTransformer()\n",
    "    median_feature_transformer.setInputCol('selected_features').setOutputCol('median')\n",
    "    \n",
    "    number_of_occurences_feature_transformer = NumberOfOccurrencesFeaturesTransformer()\n",
    "    number_of_occurences_feature_transformer.setInputCol('selected_features').setOutputCol('number_of_occurences')\n",
    "    \n",
    "    stages = [\n",
    "        featurer,\n",
    "        feature_selector,\n",
    "        max_feature_transformer,\n",
    "        mean_feature_transformer,\n",
    "        median_feature_transformer,\n",
    "        number_of_occurences_feature_transformer\n",
    "    ]\n",
    "        \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline = pipeline.fit(features_as_df)\n",
    "    pipeline = pipeline.transform(features_as_df)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def transform_posts(posts_as_df):\n",
    "    \n",
    "    poster = PostTransformer()\n",
    "    poster.setInputCol('content').setOutputCol('posts')\n",
    "    \n",
    "    translator = TranslateTransformer()\n",
    "    translator.setInputCol('posts').setOutputCol('translated')\n",
    "    \n",
    "    sentencer = SentenceTransformer()\n",
    "    sentencer.setInputCol('translated').setOutputCol('sentences')\n",
    "    \n",
    "    speech_parter = SpeechPartsTransformer()\n",
    "    speech_parter.setInputCol('translated').setOutputCol('speechParts')\n",
    "    \n",
    "    basic_speech_parter = BasicSpeechPartsTransformer()\n",
    "    basic_speech_parter.setInputCol('translated').setOutputCol('basicSpeechParts')\n",
    "    \n",
    "    sentimenter = SentimentTransformer()\n",
    "    sentimenter.setInputCol('translated').setOutputCol('sentiments')\n",
    "\n",
    "    stages = [\n",
    "        poster,\n",
    "        translator, \n",
    "        sentencer, \n",
    "        speech_parter,\n",
    "        basic_speech_parter,\n",
    "        sentimenter\n",
    "    ]\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline = pipeline.fit(posts_as_df)\n",
    "    pipeline = pipeline.transform(posts_as_df)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(selected_features={'leaf': [1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 2.0], 'has-attribute-class': [1.0, 1.0, 1.0, 1.0]}, max={'leaf': 2.0, 'has-attribute-class': 1.0}, mean={'leaf': 1.1428571428571428, 'has-attribute-class': 1.0}, median={'leaf': 1.0, 'has-attribute-class': 1.0}, number_of_occurences={'leaf': 7, 'has-attribute-class': 4})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_file = 'data/featuresample.json'\n",
    "loaded_features = load_features(sc, feature_file)\n",
    "\n",
    "result = transform_features(loaded_features)\n",
    "             \n",
    "result.select('selected_features','max','mean', 'median', 'number_of_occurences').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|           sentences|          translated|         speechParts|basicSpeechParts|          sentiments|\n",
      "+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|[Oh, please ... v...|[Oh, please ... v...|Map(IN -> 20, EX ...|    [48, 43, 17]|[[0.3,0.716666666...|\n",
      "|[Oh, I'm also the...|[Oh, I'm also the...|Map(IN -> 44, RP ...|    [94, 83, 33]|[[0.1921212121212...|\n",
      "|[Tell me, what do...|[Tell me, what do...|Map(IN -> 39, PRP...|    [79, 54, 31]|[[0.25,0.75], [-0...|\n",
      "+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_files = 'data/posts/*'\n",
    "loaded_posts = load_posts(sc, post_files)\n",
    "\n",
    "post_pipeline = transform_posts(loaded_posts)\n",
    "post_pipeline = post_pipeline.select('sentences', 'translated', 'speechParts', 'basicSpeechParts', 'sentiments')\n",
    "post_pipeline.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metody pomocnicze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_output(posts_df):\n",
    "    a = posts_df.select('sentences').first().sentences[0]\n",
    "    b = posts_df.select('sentences').first().sentences[1]\n",
    "    c = posts_df.select('sentences').first().sentences[2]\n",
    "    d = posts_df.select('translated').first().translated[0]\n",
    "    e = posts_df.select('speechParts').first().speechParts\n",
    "    f = posts_df.select('sentiments').first().sentiments[0]\n",
    "    g = posts_df.select('sentiments').first().sentiments[1]\n",
    "    h = posts_df.select('sentiments').first().sentiments[2]\n",
    "    scheme = '{}\\n\\n{}\\n\\n{}\\n\\n{}\\n\\n{}\\n\\n{}\\n\\n{}\\n\\n{}'\n",
    "    print(scheme.format(a,b,c,d,e,f,g,h))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
